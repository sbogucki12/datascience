{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are the configurable aspects of your algorithm that can be adjusted to improve its performance. For example, k-Nearest Neighbors has the hyperparameter “k” that determines the size of the neighborhood.\n",
    "\n",
    "One great aspect of CART trees is that essentially you can always build a tree that gets zero training error on any real world data set. The reason is if there's any two data points that have a different label we split between them.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "We adjust hyperparameters to avoid under or overfitting the training data.  \n",
    "\n",
    "A classic sign of overfitting is high test set error and low training set error.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLearningRate(learningRate, steps):\n",
    "    a = learningRate\n",
    "\n",
    "    for i in range(steps):\n",
    "        a = learningRate/steps\n",
    "        steps = steps - 1\n",
    "        print(a)\n",
    "    \n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.00101010101010101\n",
      "0.0010204081632653062\n",
      "0.0010309278350515464\n",
      "0.0010416666666666667\n",
      "0.0010526315789473684\n",
      "0.0010638297872340426\n",
      "0.001075268817204301\n",
      "0.0010869565217391304\n",
      "0.001098901098901099\n",
      "0.0011111111111111111\n",
      "0.0011235955056179776\n",
      "0.0011363636363636365\n",
      "0.0011494252873563218\n",
      "0.0011627906976744186\n",
      "0.0011764705882352942\n",
      "0.0011904761904761906\n",
      "0.0012048192771084338\n",
      "0.0012195121951219512\n",
      "0.0012345679012345679\n",
      "0.00125\n",
      "0.0012658227848101266\n",
      "0.001282051282051282\n",
      "0.0012987012987012987\n",
      "0.0013157894736842105\n",
      "0.0013333333333333335\n",
      "0.0013513513513513514\n",
      "0.0013698630136986301\n",
      "0.001388888888888889\n",
      "0.0014084507042253522\n",
      "0.0014285714285714286\n",
      "0.0014492753623188406\n",
      "0.0014705882352941176\n",
      "0.0014925373134328358\n",
      "0.0015151515151515152\n",
      "0.0015384615384615385\n",
      "0.0015625\n",
      "0.0015873015873015873\n",
      "0.0016129032258064516\n",
      "0.001639344262295082\n",
      "0.0016666666666666668\n",
      "0.0016949152542372883\n",
      "0.001724137931034483\n",
      "0.0017543859649122807\n",
      "0.0017857142857142859\n",
      "0.0018181818181818182\n",
      "0.001851851851851852\n",
      "0.0018867924528301887\n",
      "0.0019230769230769232\n",
      "0.00196078431372549\n",
      "0.002\n",
      "0.0020408163265306124\n",
      "0.0020833333333333333\n",
      "0.002127659574468085\n",
      "0.002173913043478261\n",
      "0.0022222222222222222\n",
      "0.002272727272727273\n",
      "0.002325581395348837\n",
      "0.002380952380952381\n",
      "0.0024390243902439024\n",
      "0.0025\n",
      "0.002564102564102564\n",
      "0.002631578947368421\n",
      "0.002702702702702703\n",
      "0.002777777777777778\n",
      "0.002857142857142857\n",
      "0.0029411764705882353\n",
      "0.0030303030303030303\n",
      "0.003125\n",
      "0.0032258064516129032\n",
      "0.0033333333333333335\n",
      "0.003448275862068966\n",
      "0.0035714285714285718\n",
      "0.003703703703703704\n",
      "0.0038461538461538464\n",
      "0.004\n",
      "0.004166666666666667\n",
      "0.004347826086956522\n",
      "0.004545454545454546\n",
      "0.004761904761904762\n",
      "0.005\n",
      "0.005263157894736842\n",
      "0.005555555555555556\n",
      "0.0058823529411764705\n",
      "0.00625\n",
      "0.006666666666666667\n",
      "0.0071428571428571435\n",
      "0.007692307692307693\n",
      "0.008333333333333333\n",
      "0.009090909090909092\n",
      "0.01\n",
      "0.011111111111111112\n",
      "0.0125\n",
      "0.014285714285714287\n",
      "0.016666666666666666\n",
      "0.02\n",
      "0.025\n",
      "0.03333333333333333\n",
      "0.05\n",
      "0.1\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(createLearningRate(0.1, 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "Both the training error and the test error will be high\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Although training error continues to decrease over time, test error will begin to increase again\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "Cross validation is a method used to estimate the testing error of a machine learning model.\n",
    "\n",
    "A common practice is to take the training data therefore and split it into two partitions: training and validation. \n",
    "\n",
    "## Pruning\n",
    "\n",
    "Pruning is a technique used to speed up the CART algorithm by finding the right balance between complexity and simplicity in a decision tree. \n",
    "\n",
    "And we build a full tree on the training data set until we get zero error. All the way, we don't worry about overfitting. And once we have this gigantic tree, we start pruning away nodes from that tree that have no or a negative effect on the error on the validation data set.\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "Grid Search is a common method for setting multiple hyperparameters\n",
    "\n",
    "We train a model for each of all possible combinations of hyperparameter values in the specified parameter grid, pick the one that gives you the lowest validation error.\n",
    "\n",
    "## Other Techniques to mitigate overfitting\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "In regularization, we attach penalties to complex model parameters that compels the model to be simpler.\n",
    "\n",
    "CLassifiers that get too complex are penalized. \n",
    "\n",
    "So, for example, for each split, we would have to assess whether the decrease in impunity is more than increase in the penality we get for introducing additional nodes. \n",
    "\n",
    "#### Early Stopping\n",
    "\n",
    "As you work through the training data, you regularly \"peek\" at the validation data.  Once you've surpassed the minimum in the validation set,  you recover the minimum vectors by undoing steps, or by recovering something that you saved onto disk, and that's the classifier that you return.\n",
    "\n",
    "In other words, in early stopping, we stop training as validation error starts to increase again.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kFold(n, k):\n",
    "    \"\"\"\n",
    "    Generates [(training_indices, validation_indices), ...] for k-fold validation.\n",
    "    \n",
    "    Input:\n",
    "        n: number of training examples\n",
    "        k: number of folds\n",
    "    \n",
    "    Output:\n",
    "        kfold_indices: a list of length k. Each entry takes the form (training indices, validation indices)\n",
    "    \"\"\"\n",
    "    assert k >= 2\n",
    "    kfold_indices = []\n",
    "    \n",
    "    indices = list(range(n))\n",
    "    fold_size = n // k\n",
    "    remainder = n % k\n",
    "    \n",
    "    start = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        fold_indices = indices[start:start+fold_size]\n",
    "        validation_indices = fold_indices.copy()\n",
    "        start += fold_size\n",
    "        \n",
    "        if remainder > 0:\n",
    "            validation_indices.append(indices[start])\n",
    "            start += 1\n",
    "            remainder -= 1\n",
    "        \n",
    "        training_indices = [idx for idx in indices if idx not in validation_indices]\n",
    "        kfold_indices.append((training_indices, validation_indices))\n",
    "    \n",
    "   \n",
    "    return kfold_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(xTr, yTr, depths, indices):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    best_loss = float('inf')\n",
    "    best_depth = None\n",
    "    \n",
    "    for fold_indices in indices:\n",
    "        training_indices, validation_indices = fold_indices\n",
    "        xTr_fold = xTr[training_indices]\n",
    "        yTr_fold = yTr[training_indices]\n",
    "        xVal_fold = xTr[validation_indices]\n",
    "        yVal_fold = yTr[validation_indices]\n",
    "        \n",
    "        fold_training_losses = []\n",
    "        fold_validation_losses = []\n",
    "        \n",
    "        for depth in depths:\n",
    "            model = DecisionTreeRegressor(max_depth=depth)\n",
    "            model.fit(xTr_fold, yTr_fold)\n",
    "            yTr_pred = model.predict(xTr_fold)\n",
    "            yVal_pred = model.predict(xVal_fold)\n",
    "            tr_loss = square_loss(yTr_fold, yTr_pred)\n",
    "            val_loss = square_loss(yVal_fold, yVal_pred)\n",
    "            fold_training_losses.append(tr_loss)\n",
    "            fold_validation_losses.append(val_loss)\n",
    "        \n",
    "        avg_training_loss = np.mean(fold_training_losses)\n",
    "        avg_validation_loss = np.mean(fold_validation_losses)\n",
    "        training_losses.append(avg_training_loss)\n",
    "        validation_losses.append(avg_validation_loss)\n",
    "        \n",
    "        if avg_validation_loss < best_loss:\n",
    "            best_loss = avg_validation_loss\n",
    "            best_depth = depths[np.argmin(fold_validation_losses)]\n",
    "    \n",
    "    return best_depth, training_losses, validation_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05307258, -2.2338863 , -0.59348454,  0.11202077, -0.76738004,\n",
       "       -1.26311451,  0.66433584,  0.91055912, -1.6646629 ,  0.56281574,\n",
       "        0.83704072, -1.4855316 ,  0.54125185,  1.10432153, -1.36060803,\n",
       "        0.35580083,  0.1210704 , -1.1964292 ,  0.26010688, -1.25826104,\n",
       "        0.41170476,  1.23538567, -1.41498507, -2.71778568,  0.83997386,\n",
       "       -1.47754469,  0.73456265,  1.58256232,  1.06842739, -0.36561153,\n",
       "       -1.2489387 ,  1.28895897,  1.56850344, -0.11643947,  0.37507747,\n",
       "        0.45292638, -0.45394039,  0.80868525, -0.46816592,  0.19007972,\n",
       "        0.28944009,  1.33937448, -0.57175225,  0.36074114,  0.60156309,\n",
       "        2.30251948, -1.17517046, -0.36540592, -1.14453543,  0.40875822,\n",
       "        1.79469481, -0.15581822, -1.13609731, -2.07397915, -0.44395563,\n",
       "       -1.50158435, -0.51508367,  0.10574328,  0.96978416, -1.97569999,\n",
       "        0.02669615,  0.02376358,  0.51097562, -0.78536143, -1.43596569,\n",
       "       -1.11454219, -1.44983201, -0.60624508, -1.14518402,  1.5057244 ,\n",
       "       -1.14280876,  0.30893454, -2.46105882, -1.68174131,  0.52302766,\n",
       "        0.07321937,  0.23914568, -0.32378518, -1.20800835,  1.74841797,\n",
       "        2.85289279,  0.45752562,  0.33814405, -1.96878404, -0.31277232,\n",
       "       -1.69432309,  0.58269755,  0.45510805,  1.22225418,  1.13716495,\n",
       "        0.90536611,  2.30793566, -0.08903982,  0.57878535, -1.14692034,\n",
       "       -2.35028481, -0.12213542, -0.98642118, -0.30140572, -2.12350832])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
