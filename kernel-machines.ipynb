{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Kernel Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maximum margin classifiers\n",
    "\n",
    "In order to generalize your model to work well with new data, you will use the \"maximum margin principle,\" which stipulates that the best hyperplane is the one that maximizes the distance between the closest data points on either side of the boundary. In order to find such a hyperplane, you will use Support Vector Machines (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Margin Hyperplane. \n",
    "\n",
    "You can have infinite hyperplanes splitting sets of positive and negative points, \n",
    "\n",
    "The Maximum Margin Hyperplane is exactly in the middle of the two sets. \n",
    "\n",
    "So that when new data is introduced, there is more room to fit in points on the appropriate side of the hyperplane. \n",
    "\n",
    "(If the hyperplane was close to points in either set of the training data, then there will more likelihood of mislabeling when new data is introduced)\n",
    "\n",
    "*gamma* is th distance from the hyperplane to the closest points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the maximum margin\n",
    "\n",
    "1. Formulate distance of a hyperplane to a point. \n",
    "   1.  W transpose (x + b) = 0\n",
    "       1.  w is a vector that is orthogonal to the hyperplane\n",
    "\n",
    "\n",
    "we have a point, x. \n",
    "How far is x from the hyperplane? \n",
    "d is a hyperplane that goes straight from the hyperplane to point x\n",
    "length of vector d is the distance from point x to the hyperplane \n",
    "the length of a vector is the *L2 norm*, sqrt(d transpose d)\n",
    "x(p) us the projection of x on the hyperplane (so the point on the hypeplane that is the L2 norm of vector d away from x)\n",
    "\n",
    "note: all points on the hyperplane must satisfy w transpose x(p) + b = 0\n",
    "note: x(p) = x - d\n",
    "note: d is orthogonal to the hyperplane (just like w), making d a rescaled version of w, which makes: \n",
    "w transpose (x - alpha(w)) + b = 0\n",
    "Alpha scales w\n",
    "solve for alpha: \n",
    "a = w transpose (x + b) divided by w transpose w\n",
    "d = a*w\n",
    "length of d is sqrt(d transpose d)\n",
    "which can be tranlated as: \n",
    "d = sqrt(pwr(a, 2) * (w transpose w))\n",
    "which be translated as: \n",
    "d = abs(w transpose ( x + b)) divided by sqrt(w transpose w)\n",
    "sqrt(w transpose w) === linalg.norm(w)\n",
    "\n",
    "Gamma is what are trying to maximize in SVM: \n",
    "\n",
    "gamma(w, b) = min over all possible points x(abs(w.dot(x + b)) divided by linalg.norm(w))\n",
    "\n",
    "so, we can replace x with: \n",
    "x transpose (x - d) + b = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min of the absolute value of the w vector transpose the point x (parameter) plus b.  \n",
    "\n",
    "scipy.optimize.minimize(abs(w.dot(x + b)))\n",
    "\n",
    "note: w dot w is the magnitude of w (the Euclidian Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that provides the mininim distance over all possible points x(abs(w.dot(x + b)) divided by linalg.norm(w))\n",
    "def distance(w, b, x):\n",
    "    return np.abs(w.dot(x) + b) / np.linalg.norm(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that the constraint works is: \n",
    "\n",
    ">= 1 means that every result has to be positive, so: \n",
    "\n",
    "-1 label has to have -1 prediction\n",
    "1 label has to have 1 prediction \n",
    "\n",
    "(opposite signs would result in negative -)\n",
    "\n",
    "note: as we shrink w, the distance to x (d) increases\n",
    "\n",
    "the solution to SVM is when you maximize the distance (d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to use **slack variables** when no straight hyperplane exists (because the data is not linearly separable)\n",
    "\n",
    "psi(i) is a positive term that we want to make as small as possible. \n",
    "\n",
    "we are going to minimize the sum of all psi(i)\n",
    "\n",
    "lowering c allows for some \"slack\", which provides some leeway for data points that aren't linearly separable (mislabeled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can remove constraints from optimization problem.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as optimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "optimize.optimize(1 - y[i] *(x.dot(w) + b), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
