{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Models\n",
    "\n",
    "take a sequence as input, return a sequence as output\n",
    "\n",
    "encoder == turns the input into a multidimensional vector\n",
    "\n",
    "decoder == does something specific and then outputs a sequence\n",
    "\n",
    "Encoders take in a sequence of words and outputs a vector (or a code) that can be viewed as a summary of the input sequence.\n",
    "\n",
    "Decoders take in such a vector (computed from encoder) and turn it into a scalar or sequence of outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embdeddings\n",
    "\n",
    "Word embeddings are generating by ingesting massive amounts of text data and determining probabilistic relationships between any given words and surrounding words. \n",
    "\n",
    "Typically, the inner product of two word vectors is used as input to a likelihood function to estimate how likely it is that these two words co-occur. \n",
    "\n",
    "train model by taking a huge corpus of text and then go through, a sequence of words at a time, and try to predict the relationship (by looking at works 0,1,3,4, what is word 2?)\n",
    "\n",
    "- to find the relationship among words, get the inner product of the two word vectors: w(i).transpose(w(j))\n",
    "  - if inner product is high, the two words have a high probability of occurring together, and low product means low probability of occurring together\n",
    "\n",
    "Products are not probabilities.  We use softmax to get the probabilities. \n",
    "\n",
    "Pseudo code: probabilities = expontiated inner product / exponential word vector of all other words.transpose(W(J))\n",
    "\n",
    "![convolution.jpg](sequence_probabilities.jpg)\n",
    "\n",
    "Given a sequence of m words, the log likelihood of observing this sequence is ![convolution.jpg](sequence_log_probabilities.jpg)\n",
    "\n",
    "Then, maximize the log likelihood function, or equivalently minimize the negative log likelihood function by using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Use a deep averaging network\n",
    "\n",
    "average all the vectors to get a single vector then put that vector as input into a neural network that determines classification: positive or negative\n",
    "\n",
    "When we back propagate through the neural network, we add the step of updating the word embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks (RNNs) are neural networks that are applied over and over again at each element of a sequence to produce a summary vector (in the encoder) or an output sequence (in the decoder).\n",
    "\n",
    "Just like regular neural networks, you take every layer times the output of the previous layer multiplied by the weights matrix, but also every layer gets its own input, which also get multiplied by a weight matrix (also, the weight matrices are the same in every layer), AND every layer can have an output \n",
    "\n",
    "Encoder: \n",
    "\n",
    "![recurrent.jpg](recurrent.jpg)\n",
    "\n",
    "Decoder: \n",
    "\n",
    "![rnn_decoder.jpg](rnn_decoder.jpg)\n",
    "\n",
    "1. We process the previous hidden state h(i−1). This is done with matrix multiplication to yield W*hi−1. This represents the “history” component of the sentence before the word s(i).\n",
    "2. Process the current input s(i) by using matrix multiplication to yield U*s(i). This represents the information from the current input s(i). \n",
    "3. Add up the the output of the previous two steps to yield W\\*h(i−1)+U\\*s(i).\n",
    "4. Apply a nonlinear transition function σ (such as ReLU) to the output of (3) to yield the hidden state hi=σ(W\\*h(i−1)+U\\*s(i)).\n",
    "(Optional) Depending on the application, we can further process the hidden state variable oi.\n",
    "\n",
    "For the decoder, to input a vector instead of a sequence, you use the output of the encoder as initial hidden state and input a starting word for the sentence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
