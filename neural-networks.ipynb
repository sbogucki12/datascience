{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear functions\n",
    "\n",
    "\n",
    "- multi-layer Perceptron == artificial neural network instead of learning one function, it learns many linear functions \n",
    "- linear function == w Transpose x (this particular function goes through the origin) \n",
    "- adding offset b shifts the line from the origin, so w Transpose x + b\n",
    "- if we want to add angles to the line, we use a hinge function max(w Transpose x +b, 0) == hinge function\n",
    "- Multiply the function by a scalar and add a constant to hinge the line downward (negative) u \\* max(w Transpose x +b, 0) + c\n",
    "- Keep adding on hinge functions with different scalars to get the desired curve. \n",
    "- so a neural network is the function h(x), which is Sum up all the hinge functions, including their scalars, take the max(w Transpose x + b, 0) + one constant\n",
    "- In matrix format, W, b and the scalar are all now vectors scalar Transpose max(W + b, 0) plus the constant \n",
    "- Use Stochastic Gradient Descent to turn the weights, scalar, and offset into vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "- Add to the h(x) a sigma sigma os max(a, 0) \n",
    "- The **non-deep learning** way to get better apprixmation of the nonlinear function is to add more and more dimensions\n",
    "- The deep learning way to get better approximation of the nonlinear function is add more and more hidden layers. \n",
    "- Hidden layers contain neurons that correspond to hinge functions\n",
    "- The layers create \"kinks\" in the line at an exponential rate. Each layer can have four hidden neurons each, but they add 4, 16, 32, 64... kinks. So if we want to have a million kinks, we would just need 20 layers of four hidden neurons each, so that's just 80 hidden neurons.\n",
    "- Neural networks use piecewise linear functions to approximate decision boundaries. \n",
    "- It is possible to approximate any linear or nonlinear function. \n",
    "- Using fewer parameters allows for better generalizations.\n",
    "- To make networks more powerful, you either increase the number of linear functions or add more layers. \n",
    "- Adding more layers: \n",
    "  - h(x)=Wlσ(Wl−1σ(⋯W2σ(W1x)⋯))\n",
    "  - The first layer in the network learns piecewise linear functions to approximate nonlinear boundaries. It does this in two steps: \n",
    "  - W1x: This learns M linear functions. \n",
    "  - σ(W1x): This sets every negative value equal to 0, creating the kinks.\n",
    "  - Every subsequent layer in the network essentially does two things: \n",
    "  - Combines linear combinations of the functions of the previous layer and aggregates them together. This is represented by Wkσ(Wk−1)\n",
    "  - Set every negative value from Wkσ(Wk−1) to equal 0. This is represented by σ(Wkσ(Wk−1))\n",
    "- Wl == W is the accumulating linear equations and l is the number of linear equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Functions\n",
    "\n",
    "the hinge function that we have been using (that creates the \"kink\" by setting a linear function to zero) is called a REctified Linear Unit (ReLU)\n",
    "ReLU mostly replaced the sigmoid and tanh transition functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss Function\n",
    "\n",
    "Neural networks, like any other model, require a loss function to optimize\n",
    "the loss functions comes in the last layer\n",
    "softmax transition function turns outputs into probabilities (this is different than ReLU)\n",
    "the probabilities work like this: \n",
    "\n",
    "Say, the input is a dog, and three outputs are 10% cat, 20% dog, 70% cow, then we want to train the model to decrease probability of cat and cow and increase dog. \n",
    "We do this by maximizing the probability of the correct label for every single input\n",
    "To deal with the products of the probabilities, take the log.  And since we want to minimize not mazimize, we make the log negative.   \n",
    "\n",
    "Neural networks can be used for regression and classification.\n",
    "The output of a regression network is a real number or vector of real numbers. \n",
    "The output of the neural network for classification is a normalized probability vector.\n",
    "\n",
    "One dimensional regression has a one dimensional output (though, we can do regressions on more dimensions)\n",
    "Classification problems output \"belief vectors\" (also known as logits)\n",
    "Every element in the belief vector corresponds to the model's belief that the input example belongs to a certain class. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
