{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear functions\n",
    "\n",
    "\n",
    "- multi-layer Perceptron == artificial neural network instead of learning one function, it learns many linear functions \n",
    "- linear function == w Transpose x (this particular function goes through the origin) \n",
    "- adding offset b shifts the line from the origin, so w Transpose x + b\n",
    "- if we want to add angles to the line, we use a hinge function max(w Transpose x +b, 0) == hinge function\n",
    "- Multiply the function by a scalar and add a constant to hinge the line downward (negative) u \\* max(w Transpose x +b, 0) + c\n",
    "- Keep adding on hinge functions with different scalars to get the desired curve. \n",
    "- so a neural network is the function h(x), which is Sum up all the hinge functions, including their scalars, take the max(w Transpose x + b, 0) + one constant\n",
    "- In matrix format, W, b and the scalar are all now vectors scalar Transpose max(W + b, 0) plus the constant \n",
    "- Use Stochastic Gradient Descent to turn the weights, scalar, and offset into vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "\n",
    "\n",
    "- Add to the h(x) a sigma sigma os max(a, 0) \n",
    "- The **non-deep learning** way to get better apprixmation of the nonlinear function is to add more and more dimensions\n",
    "- The deep learning way to get better approximation of the nonlinear function is add more and more hidden layers. \n",
    "- Hidden layers contain neurons that correspond to hinge functions\n",
    "- The layers create \"kinks\" in the line at an exponential rate. Each layer can have four hidden neurons each, but they add 4, 16, 32, 64... kinks. So if we want to have a million kinks, we would just need 20 layers of four hidden neurons each, so that's just 80 hidden neurons.\n",
    "- Neural networks use piecewise linear functions to approximate decision boundaries. \n",
    "- It is possible to approximate any linear or nonlinear function. \n",
    "- Using fewer parameters allows for better generalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
