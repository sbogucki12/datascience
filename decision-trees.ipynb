{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a tree, you want each partition to become “purer” (i.e., containing only data from a single class). If your partitions are pure, you can easily and confidently assign labels to new data points that lie within a partition. You use an impurity metric to measure a partition’s purity compared to other partitions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity:\n",
    "    0 == all labels are 0\n",
    "    .5 == half labels are 0, other half are 1\n",
    "    1 == all are one\n",
    "\n",
    "Impurity reaches a maximum value for a set of labels that are evenly split between all possible values.\n",
    "\n",
    "Intuitively, the impurity of a set of points is higher when the points have many different labels and lower when most points have the same label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntropy(prob1, prob2):\n",
    "    '''\n",
    "        Returns the entropy of a binary variable with two possible outcomes.\n",
    "    '''\n",
    "    first_coefficient = prob1\n",
    "    negative_coefficient = -(first_coefficient)\n",
    "    logBase2_of_first_coeefficient = np.log2(first_coefficient)\n",
    "    first_term = negative_coefficient * logBase2_of_first_coeefficient\n",
    "\n",
    "    second_coefficient = prob2\n",
    "    # negative_second_coefficient = -(second_coefficient)\n",
    "    logBase2_of_second_coefficient = np.log2(second_coefficient)\n",
    "    second_term = second_coefficient * logBase2_of_second_coefficient\n",
    "\n",
    "    result = first_term - second_term\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootEntropy = getEntropy(16/30, 14/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanceGreaterThan50k = getEntropy(12/13, 1/13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39124356362925566"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanceGreaterThan50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanceGreaterThanOrEqualTo50k = getEntropy(4/17, 13/17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7871265862012691"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanceGreaterThanOrEqualTo50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are whether individuals are likely to pay off a debt or not\n",
    "# features are the balance of the individual's account and whether they rent or own their residence\n",
    "# splitting on balance < 50k reduces the entropy by 0.38121435556157335\n",
    "\n",
    "entropy = (13/30) * balanceGreaterThan50k + (17/30) * balanceGreaterThanOrEqualTo50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6155772764200632"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropyReduction = rootEntropy - entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38121435556157335"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropyReduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering a split, you take a weighted average of the purity of the points across the new nodes.\n",
    "\n",
    "You want to split the data such that the average purity in each of the new nodes is highest.\n",
    "\n",
    "The optimal split is the one that minimizes the probability-weighted impurity in the new sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the CART algorithm builds a classification tree by iterating through all possible binary partitions of the training data based on a single feature threshold. It then picks the one partition that leads to the best weighted impurity of the resulting children. This procedure is repeated on each new dataset resulting from each partition until a stopping criterion is met. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CART algorithm stops in exactly two cases:\n",
    "\n",
    "If all data points in the dataset share the same label, we stop splitting and create a leaf with label y\n",
    ".\n",
    "If all data points in the dataset share the same features, we create a leaf with the most common label y\n",
    " for classification and average label for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If all data points in the dataset share the same label, we stop splitting and create a leaf with label y\n",
    "class Node:\n",
    "  def __init__(self, label):\n",
    "    self.label = label   \n",
    "\n",
    "  def __str__(self):\n",
    "    return f\"{self.label}\"\n",
    "\n",
    "def checkForUniqueLabels(dataset):\n",
    "    if len(np.unique(dataset.labels)) == 1:\n",
    "        node = Node(\"y\")\n",
    "        return node\n",
    "    if len(np.unique(dataset)) > 1:\n",
    "        node = Node(\"n\")\n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creata Classification and Refression Tree (CART) algorithm\n",
    "# 1. Check if the dataset is pure, if so create a leaf node with the label\n",
    "# 2. Check if the dataset is empty, if so create a leaf node with the most common label\n",
    "# 3. If the dataset is not pure or empty, try partitioning the dataset on each of the unique attribute values\n",
    "# 4. Compute the entropy of the two resulting datasets using the weighted entropy\n",
    "# 5. Return the question that produces the lowest entropy, as well as the two datasets it yields\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, question=None, true_branch=None, false_branch=None, leaf_label=None):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "        self.leaf_label = leaf_label\n",
    "\n",
    "class Question:\n",
    "    def __init__(self, feature_index, threshold):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def match(self, example):\n",
    "        value = example[self.feature_index]\n",
    "        return value <= self.threshold\n",
    "\n",
    "def CART(features, labels, task='classification'):\n",
    "    if task == 'classification':\n",
    "        impurity_func = calculate_gini_impurity\n",
    "        leaf_label_func = most_common_label\n",
    "    elif task == 'regression':\n",
    "        impurity_func = calculate_variance\n",
    "        leaf_label_func = average_label\n",
    "\n",
    "    if all_same(labels):\n",
    "        return Node(leaf_label=labels[0])\n",
    "\n",
    "    if len(features) == 0:\n",
    "        return Node(leaf_label=leaf_label_func(labels))\n",
    "\n",
    "    best_question, true_indices, false_indices = find_best_split(features, labels, impurity_func)\n",
    "    true_branch = CART(features[true_indices], labels[true_indices], task)\n",
    "    false_branch = CART(features[false_indices], labels[false_indices], task)\n",
    "    return Node(question=best_question, true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "def find_best_split(features, labels, impurity_func):\n",
    "    best_question = None\n",
    "    best_impurity = float('inf')\n",
    "    true_indices = None\n",
    "    false_indices = None\n",
    "\n",
    "    for feature_index in range(features.shape[1]):\n",
    "        unique_values = np.unique(features[:, feature_index])\n",
    "\n",
    "        for threshold in unique_values:\n",
    "            question = Question(feature_index, threshold)\n",
    "            true_indices_mask = question.match(features)\n",
    "            false_indices_mask = np.logical_not(true_indices_mask)\n",
    "\n",
    "            true_labels = labels[true_indices_mask]\n",
    "            false_labels = labels[false_indices_mask]\n",
    "\n",
    "            impurity = weighted_impurity(true_labels, false_labels, impurity_func)\n",
    "\n",
    "            if impurity < best_impurity:\n",
    "                best_impurity = impurity\n",
    "                best_question = question\n",
    "                true_indices = true_indices_mask\n",
    "                false_indices = false_indices_mask\n",
    "\n",
    "    return best_question, true_indices, false_indices\n",
    "\n",
    "def weighted_impurity(true_labels, false_labels, impurity_func):\n",
    "    total_samples = len(true_labels) + len(false_labels)\n",
    "    true_weight = len(true_labels) / total_samples\n",
    "    false_weight = len(false_labels) / total_samples\n",
    "    impurity = (true_weight * impurity_func(true_labels)) + (false_weight * impurity_func(false_labels))\n",
    "    return impurity\n",
    "\n",
    "def calculate_gini_impurity(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    impurity = 1.0\n",
    "    total_samples = len(labels)\n",
    "\n",
    "    for label in label_counts:\n",
    "        probability = label_counts[label] / total_samples\n",
    "        impurity -= probability ** 2\n",
    "\n",
    "    return impurity\n",
    "\n",
    "def calculate_variance(labels):\n",
    "    mean = np.mean(labels)\n",
    "    variance = np.mean((labels - mean) ** 2)\n",
    "    return variance\n",
    "\n",
    "def all_same(items):\n",
    "    return all(x == items[0] for x in items)\n",
    "\n",
    "def most_common_label(labels):\n",
    "    label_counts = Counter(labels)\n",
    "    most_common = label_counts.most_common(1)\n",
    "    return most_common[0][0]\n",
    "\n",
    "def average_label(labels):\n",
    "    return np.mean(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typically, in regression, all the labels have different values.\n",
    "\n",
    "# Regression Tree\n",
    "\n",
    "In each partition, predict the average label of all the training points that fall into that partition.\n",
    "\n",
    "The second thing is we need a different impurity metric. We can use \"Square-Loss\". \n",
    "- For each partition:\n",
    "  - compute the average of all the labels in that partition\n",
    "  - compute the impurity as what's the squared difference of every single label from that average on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        \n",
    "    def square_loss(self, labels):\n",
    "        average_label = np.mean(labels)\n",
    "        squared_diff = np.mean((labels - average_label) ** 2)\n",
    "        return squared_diff\n",
    "    \n",
    "    def split_dataset(self, X, y, feature_index, threshold):\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[right_mask], y[right_mask]\n",
    "        return X_left, y_left, X_right, y_right\n",
    "    \n",
    "    def find_best_split(self, X, y):\n",
    "        best_impurity = np.inf\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                X_left, y_left, X_right, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
    "                impurity_left = self.square_loss(y_left)\n",
    "                impurity_right = self.square_loss(y_right)\n",
    "                \n",
    "                total_samples = y.shape[0]\n",
    "                weighted_impurity = (impurity_left * y_left.shape[0] + impurity_right * y_right.shape[0]) / total_samples\n",
    "                \n",
    "                if weighted_impurity < best_impurity:\n",
    "                    best_impurity = weighted_impurity\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature_index, best_threshold\n",
    "    \n",
    "    def build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or np.unique(y).shape[0] == 1:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        feature_index, threshold = self.find_best_split(X, y)\n",
    "        if feature_index is None or threshold is None:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        X_left, y_left, X_right, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
    "        \n",
    "        node = {}\n",
    "        node['feature_index'] = feature_index\n",
    "        node['threshold'] = threshold\n",
    "        node['left'] = self.build_tree(X_left, y_left, depth + 1)\n",
    "        node['right'] = self.build_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y, 0)\n",
    "        \n",
    "    def predict_sample(self, sample):\n",
    "        node = self.tree\n",
    "        while isinstance(node, dict):\n",
    "            if sample[node['feature_index']] <= node['threshold']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        return node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(sample) for sample in X])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted value on a test point is the average of values in a leaf of the tree.\n",
    "\n",
    "Redefine impurity such that it captures the \"spread\" in values within a given subset of data.\n",
    "Because labels are no longer categorical, we redefine impurity such that it captures the \"spread\" of values in each node.\n",
    "Capture this spread within a set by the variance of the labels within the set.\n",
    "\n",
    "Seek to minimize the spread of values at each successive node of the tree.\n",
    "\n",
    "When we build a CART tree, we essentially try every single split in every single dimension. For each split, we compute the impurity on the right side and on the left side, and we take a weighted average of those two impurity scores. \n",
    "\n",
    "The square loss is the sum of all the data points in that side and we compute the difference between the label and the average label and we square the whole thing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = number of points on right side\n",
    "when we move a point away from the right side to the left side, then we just decrement n on the right side and we increase it on the left side\n",
    "s = sum of labels on right side and left\n",
    "if you have a data point in the right side you want to move it to the left side, we just subtract it on the right, move it, add it to the left. \n",
    "q = sum of squared labels \n",
    "if you have it on the right side and we move something, we just subtract a squared label of this data point and add it to the other side.\n",
    "\n",
    "sum over all the y i squared minus two times y bar times the sum over y i and then plus sum over y bar squared. \n",
    "substitute an s over n every time you see y bar.\n",
    "Simplify: \n",
    "Q minus s squared over n is the entire impurity on one side.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
