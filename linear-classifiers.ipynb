{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w = sum of the list of points that were misclassified (repetition allowed) in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These vectors are examples of misclassified points.  Sum all of these to update the perceptron.  \n",
    "\n",
    "myList = np.array([[0,0,0,0,4],\n",
    "[0,0,0,0,4],\n",
    "[0,0,6,5,0],\n",
    "np.negative([3,0,0,0,0]),\n",
    "np.negative([0,9,3,6,0]),\n",
    "np.negative([0,1,0,2,5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  4],\n",
       "       [ 0,  0,  0,  0,  4],\n",
       "       [ 0,  0,  6,  5,  0],\n",
       "       [-3,  0,  0,  0,  0],\n",
       "       [ 0, -9, -3, -6,  0],\n",
       "       [ 0, -1,  0, -2, -5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3, -10,   3,  -3,   3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.dot([0,0, 1],[1,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.10.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib \n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "##from helper import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\t= -1\n",
      "x\t= [-2 -1 -2  1 -2  0  0 -2 -1  0]\n",
      "w_old\t= [ 1 -1 -1  1 -1 -1  1  1 -1  1]\n",
      "w_new\t= [ 3  0  1  0  1 -1  1  3  0  1]\n"
     ]
    }
   ],
   "source": [
    "def perceptron_update(x,y,w):\n",
    "    \"\"\"\n",
    "    function w=perceptron_update(x,y,w);\n",
    "    \n",
    "    Updates the perceptron weight vector w using x and y\n",
    "    Input:\n",
    "    x : input vector of d dimensions (d)\n",
    "    y : corresponding label (-1 or +1)\n",
    "    w : weight vector of d dimensions\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector after updating (d)\n",
    "    \"\"\"\n",
    "    w = w + (y*x)\n",
    "    return w\n",
    "    \n",
    "# little test\n",
    "y = -1\n",
    "x = np.random.randint(low=-2, high=2, size=10)\n",
    "w = np.random.randint(low=0, high=2, size=10)*2 - 1\n",
    "print(f'y\\t= {y}')\n",
    "print(f'x\\t= {x}')\n",
    "print(f'w_old\\t= {w}')\n",
    "w_new = perceptron_update(x, y, w)\n",
    "print(f'w_new\\t= {w_new}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(xs, ys):\n",
    "    \"\"\"\n",
    "    function w=perceptron(xs,ys);\n",
    "    \n",
    "    Returns the weight vector learned by the Perceptron classifier.\n",
    "    We assume that the last dimension of each xs is 1.\n",
    "    \n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd matrix)\n",
    "    ys : n labels (-1 or +1)\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector (d)\n",
    "    b : bias term\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = xs.shape     # so we have n input vectors, of d dimensions each\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "\n",
    "      \n",
    "    w = 0\n",
    "    iter = 0\n",
    "    '''\n",
    "    print('hello')\n",
    "    while True: \n",
    "        iter = iter + 1\n",
    "        m = 0\n",
    "        for x,y in zip(xs,ys):\n",
    "            \n",
    "            if y*(x * w)  <= 0:\n",
    "                w = perception_update(xs, ys, w)\n",
    "                m = m + 1\n",
    "        if m == 0:\n",
    "            print(f'test:{w}')\n",
    "            return w\n",
    "        elif  iter >= 100:\n",
    "            print(f'test:{w}')\n",
    "            return w\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(xs, ys):\n",
    "    \"\"\"\n",
    "    function w=perceptron(xs,ys);\n",
    "    \n",
    "    Returns the weight vector learned by the Perceptron classifier.\n",
    "    We assume that the last dimension of each xs is 1.\n",
    "    \n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd matrix)\n",
    "    ys : n labels (-1 or +1)\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector (d)\n",
    "    b : bias term\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = xs.shape     # so we have n input vectors, of d dimensions each\n",
    "    xs = np.concatenate((xs, np.ones((n, 1))), axis=1) \n",
    "    w = np.zeros(d + 1)\n",
    "    b = 0 \n",
    "    max_iterations = 100\n",
    "    converged = False\n",
    "\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        indices = np.random.permutation(n)  # Randomize the order of training data\n",
    "        converged = True  # Assume convergence\n",
    "\n",
    "        for idx in indices:\n",
    "            xi = xs[idx]\n",
    "            yi = ys[idx]\n",
    "\n",
    "            if np.dot(xi, w) * yi <= 0:\n",
    "                w += yi * xi\n",
    "                converged = False  # Not converged yet\n",
    "\n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "    return w[:-1], w[-1]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement classify_linear that applies the weight vector (with implicit bias term) to the input vector. Make sure that the predictions returned are either 1 or -1.\n",
    "'''\n",
    "def classify_linear(xs,w,b=0):\n",
    "    \"\"\"\n",
    "    function preds=classify_linear(xs,w,b)\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd matrix)\n",
    "    w : weight vector of dimensionality d\n",
    "    b : bias (scalar)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions (1xn vector)\n",
    "    \"\"\"\n",
    "    w = w.flatten()\n",
    "    preds = np.zeros(xs.shape[0])\n",
    "    preds = np.sign(np.dot(xs, w) + b)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRefactor classify_linear so that it passes these tests: \\n\\n# Run this self-test to check that your linear classifier correctly classifies the points in a linearly separable dataset\\n\\ndef test_linear1():\\n    xs = np.random.rand(50000,20)-0.5 # draw random data \\n    xs = np.hstack([ xs, np.ones((50000,1)) ])\\n    w0 = np.random.rand(20+1)\\n    w0[-1] = -0.1 # with bias -0.1\\n    ys = classify_linear(xs,w0)\\n    uniquepredictions=np.unique(ys) # check if predictions are only -1 or 1\\n    return set(uniquepredictions)==set([-1,1])\\n\\ndef test_linear2():\\n    xs = np.random.rand(1000,2)-0.5 # draw random data \\n    xs = np.hstack([ xs, np.ones((1000, 1)) ])\\n    w0 = np.array([0.5,-0.3,-0.1]) # define a random hyperplane with bias -0.1\\n    ys = np.sign(xs.dot(w0)) # assign labels according to this hyperplane (so you know it is linearly separable)\\n    return (all(np.sign(ys*classify_linear(xs,w0))==1.0))  # the original hyperplane (w0,b0) should classify all correctly\\n\\nruntest(test_linear1, 'test_linear1')\\nruntest(test_linear2, 'test_linear2')\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Refactor classify_linear so that it passes these tests: \n",
    "\n",
    "# Run this self-test to check that your linear classifier correctly classifies the points in a linearly separable dataset\n",
    "\n",
    "def test_linear1():\n",
    "    xs = np.random.rand(50000,20)-0.5 # draw random data \n",
    "    xs = np.hstack([ xs, np.ones((50000,1)) ])\n",
    "    w0 = np.random.rand(20+1)\n",
    "    w0[-1] = -0.1 # with bias -0.1\n",
    "    ys = classify_linear(xs,w0)\n",
    "    uniquepredictions=np.unique(ys) # check if predictions are only -1 or 1\n",
    "    return set(uniquepredictions)==set([-1,1])\n",
    "\n",
    "def test_linear2():\n",
    "    xs = np.random.rand(1000,2)-0.5 # draw random data \n",
    "    xs = np.hstack([ xs, np.ones((1000, 1)) ])\n",
    "    w0 = np.array([0.5,-0.3,-0.1]) # define a random hyperplane with bias -0.1\n",
    "    ys = np.sign(xs.dot(w0)) # assign labels according to this hyperplane (so you know it is linearly separable)\n",
    "    return (all(np.sign(ys*classify_linear(xs,w0))==1.0))  # the original hyperplane (w0,b0) should classify all correctly\n",
    "\n",
    "runtest(test_linear1, 'test_linear1')\n",
    "runtest(test_linear2, 'test_linear2')\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (least ordinary squares)\n",
    "\n",
    "linear regression (least ordinary squares) y = wTranspose(x) + bias (Gaussian noise)\n",
    "\n",
    "y[i] = wTranspose(x[i]) + e\n",
    "\n",
    "p of y given x[i] in respect to w\n",
    "\n",
    "the line is wTranspose(x)\n",
    "\n",
    "maximum likelihood estimation to estimate slope\n",
    "\n",
    "MLE helps you find the line that minimizes the average squared differences between the predicted and true values of the labels.\n",
    "\n",
    "The MLE expression is mathematically equivalent to the minimization of the mean squared error.\n",
    "\n",
    "X = [[x1.Transposed]...[xn.Transposed]   <---- features\n",
    "\n",
    "x = \n",
    "[\n",
    "    [x1],\n",
    "    ...\n",
    "    [xn]\n",
    "]\n",
    "\n",
    "y = [[y1]...[yn]] <---- labels\n",
    "\n",
    "y = \n",
    "[\n",
    "    [y1]\n",
    "    ...\n",
    "    [yn]\n",
    "]\n",
    "\n",
    "for every dimension, we x[i].transpose(w) - y\n",
    "\n",
    "every row of x is a feature vector\n",
    "\n",
    "loss function in terms of matrix algebra as X times w minus y squared and averaged. And so we basically have this big matrix X, and when we multiply with w, for every single dimension, we compute Xi transpose w, and we subtract y from it.\n",
    "\n",
    "X * w - y^2\n",
    "\n",
    "X * w (for every dimension), x[i]Transpose(w) - y\n",
    "\n",
    "write out the square, so then we get w transpose X transpose X times w, minus twice the cross-term y transpose Xw, plus y squared. Now, if you take the derivative, we get X transpose Xw, minus X transpose y. And if we equate it to 0, we can solve it with respect to w and obtain w equals the inverse of X transpose X, times X transpose y. And now we have the solution for w in a single line of number.\n",
    "\n",
    "w = (X.Transpose(X))^-1 * X.Transpose(y)\n",
    "\n",
    "𝐰=(𝐗𝑇𝐗)−1𝐗𝑇𝐲\n",
    "\n",
    "==\n",
    "\n",
    "np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "\n",
    "can also do: \n",
    "\n",
    "np.linalg.solve(X.T @ X, X.T @ y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistical Regression\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In sigmoid, implement the sigmoid function: 𝜎(𝑧)=11+𝑒−𝑧\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    function sigm = sigmoid(z)\n",
    "    \n",
    "    Returns the sigmoid of z.\n",
    "    \"\"\"\n",
    "    sigm = 1.0 / (1.0 + np.exp(-z))\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implement the function y_pred(X, w, b=0) that computes  𝑃(𝑦𝑖=1|𝐱𝑖;𝐰,𝑏) for each row-vector  𝐱𝑖 in the matrix X.\n",
    "'''\n",
    "def y_pred(X, w, b=0):\n",
    "    \"\"\"\n",
    "    function y_pred = y_pred(X,w,b)\n",
    "    \n",
    "    Returns the probability that y_i is 1 given x_i, parameterized by w and b.\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, w) + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now you will compute the negative log likelihood in log_loss. You are given the label vector y and the data matrix X with n data points as row vectors.The negative log likelihood ( 𝑁𝐿𝐿\n",
    " ) is defined as follows:\n",
    "\n",
    "𝑁𝐿𝐿=−log𝑃(𝐲|𝐗;𝐰,𝑏)=−log∏𝑖=1𝑛𝑃(𝑦𝑖|𝐱𝑖;𝐰,𝑏)=−∑𝑖=1𝑛log𝑃(𝑦𝑖|𝐱𝑖;𝐰,𝑏)=−∑𝑖=1𝑛log𝜎(𝑦𝑖(𝐰⊤𝐱𝑖+𝑏)).\n",
    "\n",
    "While we only computed the probability of a positive label in y_pred, now we will account for the actual  𝑦\n",
    "  value. You can use your implementation of y_pred for log_loss or reimplement to account for the  𝑦\n",
    "  -- the latter yields cleaner code.\n",
    "      Calculates the negative log likelihood for dataset (X, y) using the weight vector w and bias b.\n",
    "    \n",
    "    Input:\n",
    "        X: data matrix of shape nxd\n",
    "        y: n-dimensional vector of labels (+1 or -1)\n",
    "        w: d-dimensional vector\n",
    "        b: scalar (optional, default is 0)\n",
    "    \n",
    "    Output must be a scalar\n",
    "       \n",
    "'''\n",
    "def log_loss(X, y, w, b=0):\n",
    "    \"\"\"\n",
    "    function loss = log_loss(X,y,w,b);\n",
    "    \n",
    "    Calculates the negative log likelihood for dataset (X,y)\n",
    "    using the weight vector w and bias b\n",
    "    \"\"\"\n",
    "    assert np.sum(np.abs(y)) == len(y) # check if all labels in y are either +1 or -1\n",
    "    loss = 0\n",
    "    for i in range(len(y)):\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0a966d4a9414ce6ed4a96b8eebb0cd16e252b5d68ff2097c65d5398f9c486d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
