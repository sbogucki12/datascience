{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Minimization in non-linear problems\n",
    "\n",
    "## feature expansion\n",
    "\n",
    "We use feature expansion to map the data to a higher dimensional space in order to enable a linear hyperplane to separate the data.\n",
    "\n",
    "The expression x(2/1) + x(2/2) - aka the sum of the squares of two existing features, typically represents adding a new feature that is derived from two existing features x1 and x2 Here, for each instance in your dataset, you're taking the square of the value of feature x1 adding it to the square of the value of feature x2, and using this sum as a new feature.  \n",
    "\n",
    "## phi(x)\n",
    "\n",
    "Advantage: It is simple, and your problem stays convex and well behaved. Thus, you can still use gradient descent to solve the optimization problem, just in a higher higher dimensional vector space.\n",
    "\n",
    "the problem with adding dimensions by creating new features from the sum of the squares of two other features is that it can add a lot of dimensions exponentially, degrading performance. \n",
    "\n",
    "## Dual Problem\n",
    "\n",
    "The \"dual\" of a quadratic optimization problem is a different optimization problem that results in the same solution as the original problem. You can thus take the SVM quadratic problem, solve its dual problem and obtain the exact same answer. More importantly, writing the SVM problem in its dual form lets us convert the linear problem into a non-linear problem using the \"kernel trick\". \n",
    "\n",
    "## Kernel trick\n",
    "\n",
    "phi of x transpose phi of z\n",
    "\n",
    "Implicitly maps each data point's feature vector to a higher dimension by computing inner products of data points quickly.\n",
    "\n",
    "A kernel function computes the inner product between two points in the transformed space without explicitly computing the coordinates of the points in the high-dimensional space. This is highly beneficial because it avoids the computationally expensive step of computing these coordinates explicitly, which might even be infinite-dimensional in some cases.\n",
    "\n",
    "**uses inner product (dot product of features and labels) to implicitly map to higher dimension space**\n",
    "\n",
    "the explicit sum of 2d terms becomes the product of d terms. Hence, we can compute the inner-product from the above formula in the order of d operations instead of the order of 2d\n",
    "\n",
    "Common choices for kernel functions in SVMs include:\n",
    "\n",
    "Linear: K(x, y) = x^T y\n",
    "*k = np.dot(x(i), x(j))*\n",
    "*k = np.dot(xTr, yTr)\n",
    "\n",
    "Polynomial: K(x, y) = (x^T y + c)^d\n",
    "Radial basis function (RBF)/Gaussian: K(x, y) = exp(-γ ||x-y||^2)\n",
    "\n",
    "in gradient descent problems, the inner product is w = i = 1 -> n, sum(a[i] * x[i])\n",
    "\n",
    "a = is the weight (descending in a gradient)\n",
    "\n",
    "### Linear Kernel: \n",
    "\n",
    "The linear kernel is the simplest type of kernel function. It is given by the inner product of two input vectors and can be represented mathematically as K(x, y) = x^T y. This essentially means that no transformation of the data is done and it corresponds to a linear classifier in the original feature space.\n",
    "\n",
    "### Radial Basis Function (RBF) Kernel: \n",
    "\n",
    "The RBF kernel is also known as the Gaussian kernel and is a popular choice due to its locality and scale invariance. It's often used in SVMs. The RBF kernel is defined as K(x, y) = exp(-γ||x-y||^2), where γ is a parameter that determines the spread of the function. With the RBF kernel, the higher the value of γ, the less influence far points have, effectively creating a 'local' kernel where the region of influence of each data point is limited.\n",
    "\n",
    "### Polynomial Kernel: \n",
    "\n",
    "The polynomial kernel is a more generalized type of kernel. It can model complex, non-linear relationships. It's given by K(x, y) = (x^T y + c)^d, where c is a non-negative constant and d is the degree of the polynomial. For d=1 and c=0, the polynomial kernel is equivalent to the linear kernel. As the degree increases, the polynomial kernel becomes capable of modeling more complex relationships, but can also risk overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly, *C* is the penalty for misclassifying one of your traininf data points.  \n",
    "By increasing *C*, the SVM will try harder to fit all of your data.  \n",
    "Decreasing *C* allows the SVM to make mistakes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (RBF)  Kernel sigma \n",
    "\n",
    "When sigma is large, the influence of the Euclidean distance between the input vectors is reduced, which makes the RDF kernel more similar to a linear kernel.  \n",
    "This can lead to a model with high bias but low variance (underfitting).\n",
    "\n",
    "When sigma is small, the kernel is very sensitve to the Euclidean distance between the input vectors.  \n",
    "Points that are far apart in the inpuit space will have a kernel value close to zero.  \n",
    "This can lead to a model with with low bias but high variance (overfitting)\n",
    "\n",
    "*Thus, the choice of sigma in an RBF kernel is a trade-off between bias and variance, and finding a good value often involves cross-validation or other types of hyperparameter tuning.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
