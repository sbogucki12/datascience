{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging \n",
    "\n",
    "Bagging is an ensemble method that data scientists use to improve their predictive models.\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping an approach to estimate the variance of a \n",
    "\n",
    "In a perfect case, we would calculate h-bar by training multiple data sets.  But, typically, we only have one training set.  This is where bootstrapping helps. \n",
    "\n",
    "Imagine you have a data set of N data points. This data set is drawn from the original data distribution, which you don't know.\n",
    "\n",
    "The first thing is you create new data sets, and you create artificial data sets by resampling the data that you have. And the way you do this is you just sample with replacement.\n",
    "\n",
    "In other words, reachinto the bag of data and grab a data point, add it to the new data set, and then return (replace) the data point back into the bag.  Repeat n times. \n",
    "\n",
    "New data set will have the same number of data points as the original set, but the sets are not identical. \n",
    "\n",
    "You can create many new data sets.\n",
    "\n",
    "Now you can train your classifer on each of these new data sets. \n",
    "\n",
    "This will provide a value that is little lower than the true estimated value of variance. \n",
    "\n",
    "Train the classifer on each data seta, and then compute the average classifier. \n",
    "\n",
    "h-hat is the average of the classifiers\n",
    "\n",
    "h~ is an ansemble of classifiers\n",
    "\n",
    "When you create new training data sets via bagging, new validation sets are created from the data points that weren't selected. \n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Like bagging, except for each tree, you only analyse an N subset of dimensions\n",
    "\n",
    "Pick N number of dimensions randomly for each split\n",
    "\n",
    "Random Forest has two hyperparameters: m - how many trees to average; k - how many features to subsample\n",
    "\n",
    "**Set k to be the square root of the number of dimensions**\n",
    "\n",
    "1. Sample M data sets from D (with replacement)\n",
    "2. For each data set, randomly subsample features (without replacement)\n",
    "3. For each data set (each with a subsample of features), train a full decision tree (max depth = inf)\n",
    "4. Average the predictions of all the component trees\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
