{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "**Images are represented as  3-dimensional tensors, of dimensions height × width × 3, where the last dimension captures the color channels.**\n",
    "\n",
    "**Convolutional neural networks have the ability to filter local regions of images to recognize patterns.**\n",
    "\n",
    "if the data from an image was simply vectorized, it would be one long many-dimensional vector.  There'd be no spatial relationship among dimensions.  Filtering indentifies local patterns. \n",
    "\n",
    "images have a huge number of dimensions (i.e. each image is actuall three images: red, green, blue. In 1000x1000 px image, thats actually 3 million px)\n",
    "*fitting that that many parameters would take a ton of training data*\n",
    "*red, green, blue are \"channels\"*\n",
    "\n",
    "Convolutional Neural Networks are good for image data and time-series data\n",
    "\n",
    "![convolution.jpg](convolution.jpg)\n",
    "\n",
    "translation invariant == Translation invariance is when a system produces the same output regardless of how its input is translated. For example, a face-detector might report \"FACE FOUND\" for all three images in the top row. \n",
    "\n",
    "In Euclidean geometry, translation is a geometric transformation that means moving an image or every point in space the same distance in the same direction\n",
    "\n",
    "In computer vision, translation invariance is a result of the pooling operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "convolution: passing a filter (with learned kernel) over the image to find patterns.\n",
    "\n",
    "These filters are learned through training and optimized during back propagation. \n",
    "\n",
    "the convolved image == feature map\n",
    "\n",
    "feature map is the result of the filtering operation for all pixels\n",
    "\n",
    "Get many feature maps per layer, use the output feature maps as inputs to the next layer\n",
    "\n",
    "Different filters for each image map\n",
    "\n",
    "filters are applied to images through the convolution operation\n",
    "\n",
    "Stride is the amount by which the filter is moved by as it passes over the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "The pooling layer essentially introduces translation invariance into Convolutional Neural Networks\n",
    "\n",
    "Pooling layers = shrink (blur) the feature map\n",
    "\n",
    "One way: 2 x 2 filter across the feature map, take only the max value of the 2 x 2 square, so 4 values become one.  \n",
    "\n",
    "identifies patterns (cat eyes, cat ears, cat mouth -- for examples) without respect to where they are located in the feature map\n",
    "\n",
    "Pooling introduces translation invariance by conveying what patterns are apparent in the image, without conveying where they are exactly.\n",
    "\n",
    "Two common pooling methods are max pooling and mean pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
